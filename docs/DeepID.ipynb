{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Notebook] Deep Learning Face Representation from Predicting 10,000 Classes #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paper author: Yi Sun, Xiaogang Wang, Xiaoao Tang\n",
    "\n",
    "> This paper proposes to learn a set of high-level feature representations through deep learning, referred to as Deep hidden IDentity features (DeepID), for face verification. (*[section] Abstract*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview ##\n",
    "\n",
    "### Comparison of current face verification algorithms ###\n",
    "\n",
    "[\\[appendix\\]](#A.1.1-current-face-verification-algorithms)\n",
    "\n",
    "> The current best-performing face verification algorithms typically represent faces with over-complete low-level features, followed by shallow models [9, 29, 6] (*[section] 1.Introduction*)\n",
    "\n",
    "> we propose to learn high-level face identity features with deep models through face identification, i.e. classifying a training image into one of n identities (n ≈ 10,000 in this work). (*[section] 1.Introduction*)\n",
    "\n",
    "### DeepID - Highly compact, 160-dims features ###\n",
    "\n",
    "[\\[detail\\]](#D.1.1-DeepID)\n",
    "\n",
    "> Highly compact 160-dimensional DeepID is acquired at the end of the cascade that contain rich identity information and directly predict a much larger number (e.g., 10, 000) of identity classes. (*[section] 1.Introduction*)\n",
    "\n",
    "> The proposed features are extracted from various face regions to form complementary and over-complete representations. (*[section] Abstract*)\n",
    "\n",
    "> Any state-of-the-art classifiers can be learned based on these high-level representations for face verification. (*[section] Abstract*)\n",
    "\n",
    "### Datasets  ###\n",
    "\n",
    "[\\[detail\\]](#D.2.1-Datasets)\n",
    "\n",
    "\n",
    "|Dataset|People|Image|Size|\n",
    "| ----|----|----- |----- |\n",
    "|CelebFaces|5436|87628|250x250|\n",
    "|CelebFaces+|10177|202599|250x250|\n",
    "|LFW|5749|13233|250x250|\n",
    "\n",
    "\n",
    "### Data Processing and Face Patches ###\n",
    "\n",
    "[\\[detail\\]](#D.3.1-Data-Processing-and-Face-Patches)\n",
    "\n",
    "|Patch|Scale|Region(global, local)|Color channel|\n",
    "| ----|----|----- |----- |\n",
    "|60|3|(5+5)|RGB, Gray|\n",
    "|100|5|(5+5)|RGB, Gray|\n",
    "\n",
    "\n",
    "### Entire Face verification process -  ConvNet + Joint Bayesian ###\n",
    "\n",
    "[\\[detail: Face verification process\\]](#D.4.1-Face-verification-process) [\\[detail: ConvNet\\]](#D.4.2-ConvNet) [\\[detail: Joint Bayesian\\]](#D.4.3-Joint-Bayesian) [\\[detail: *\"neural network\"*\\]](#D.4.4-%22neural-network%22)\n",
    "\n",
    "Two components of the process:\n",
    "\n",
    "- Feature extractor: ConvNet (DeepID model)\n",
    "- Classifier: Joint Bayesian or *\"neural network\"*\n",
    "\n",
    "### Experiments ###\n",
    "\n",
    "- The classification ability of Multi-scale ConvNets\n",
    "  - [\\[paper\\]](#D.5.1.1-The-classification-ability-of-Multi-scale-ConvNets) | [\\[model_62x62_3\\]](./model_62x62_3/experiment_classification_ability_of_multi-scale_convNets/experiment_classification_ability_of_multi-scale_convNets.ipynb)\n",
    "- The effectiveness of the learned hidden representations for face verification\n",
    "  - [\\[paper\\]](#D.5.1.2-The-effectiveness-of-the-learned-hidden-representations-for-face-verification) | [\\[model_62x62_3\\]](./model_62x62_3/experiment_effectiveness_of_the_learned_hidden_representations_for_face_verification/experiment_effectiveness_of_the_learned_hidden_representations_for_face_verification.ipynb)\n",
    "- The learned features extract identity information\n",
    "  - [\\[paper\\]](#D.5.1.3-The-learned-features-extract-identity-information) | [\\[model_62x62_3\\]](./model_62x62_3/experiment_learned_features_extract_identity_information/experiment_learned_features_extract_identity_information.ipynb)\n",
    "- Various face patches combination contributes to the performance\n",
    "  - [\\[paper\\]](#D.5.1.4--Various-face-patches-combination-contributes-to-the-performance)\n",
    "- Method comparison\n",
    "  - [\\[paper\\]](#D.5.1.5-Method-comparison) | [\\[model_62x62_3\\]](./model_62x62_3/experiment_method_comparison/experiment_method_comparison.ipynb)\n",
    "\n",
    "### Conclusion ###\n",
    "\n",
    "> Our method achieves 97.45% face verification accuracy on LFW using only weakly aligned faces (*[section] 1.Introduction*)\n",
    "\n",
    "> Although the prediction task at the training stage becomes more challenging, the discrimination and generalization ability of the learned features increases. (*[section] 1.Introduction*)\n",
    "\n",
    "> Allowing the DeepID to pool over multi-scale features reduces validation errors by an average of 4.72%. ([section] 4.1 Multi-scale ConvNets)\n",
    "\n",
    "> We also observe that as the number of training identities increases, the verification performance steadily gets improved. (*[section] 1.Introduction*)\n",
    "\n",
    "> We find that faces of the same identity tend to have more commonly activated neurons (positive features being in the same position) than those of different identities. ([section] 4.2 Learning effective features)\n",
    "\n",
    "> As shown in Figure 9, adding more features from various regions, scales, and color channels consistently improves the performance. ([section] 4.3 Over-complete representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detail ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D.1.1 DeepID ####\n",
    "\n",
    "[\\[back to top\\]](#DeepID---Highly-compact,-160-dims-features)\n",
    "\n",
    "**DeepID concatenation**\n",
    "\n",
    "![figure1](images/figure1.png)\n",
    "\n",
    "> We further concatenate the DeepID extracted from various face regions to form complementary and over-complete representations. (*[section] 1.Introduction*)\n",
    "\n",
    "> Each ConvNet takes a face patch as input and extracts local low-level features in the bottom layers. (*[section] 1.Introduction*)\n",
    "\n",
    "> Feature numbers continue to reduce along the feature extraction cascade while gradually more global and high-level features are formed in the top layers. (*[section] 1.Introduction*)\n",
    "\n",
    "> The learned features can be well generalized to new identities in test, which are not seen in training, and can be readily integrated with any state-of-the-art face classifiers (e.g., Joint Bayesian [8]) for face verification. (*[section] 1.Introduction*)\n",
    "\n",
    "Note:\n",
    "\n",
    "- high-level over-complete features are performed by concatenating DeepIDs of 60(or 100) ConvNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D.2.1 Datasets ####\n",
    "\n",
    "[\\[back to top\\]](#Datasets)\n",
    "\n",
    "** Dataset **\n",
    "\n",
    "LFW: 5749 people, 13233 images\n",
    "\n",
    "CelebFace: 5436 people, 87628 images\n",
    "\n",
    "CelebFace+: 10177 people, 202599 images\n",
    "\n",
    "** Dataset division **\n",
    "\n",
    "*Choice 1:*\n",
    "\n",
    "- Learning DeepID\n",
    "  - training set: 80% CelebFace (4349 people)(randomly select)\n",
    "  - validation set: 10% images of each training person (randomly select)\n",
    "- Learning Joint Bayesian\n",
    "  - training set: remaining 20% CelebFace (1400 people)\n",
    "  - testing set: all LFW pairs (6000 pairs)\n",
    "\n",
    "*Choice 2:*\n",
    "\n",
    "- Learning DeepID\n",
    "  - training set: 80% CelebFace+ (8700 people)(randomly select)\n",
    "  - validation set: 10% images of each training person (randomly select)\n",
    "- Learning Joint Bayesian\n",
    "  - training set: remaining 20% CelebFace+ (1477 people)\n",
    "  - testing set: all LFW pairs (6000 pairs)\n",
    "\n",
    "> We randomly choose 80% (4349) people from CelebFaces to learn the DeepID, and use the remaining 20% people to learn the face verification model (Joint Bayesian or neural networks). (*[section] 4. Experiments*)\n",
    "\n",
    "> We randomly select 10% images of each training person to generate the validation data. (*[section] 4. Experiments*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D.3.1 Data Processing and Face Patches ####\n",
    "\n",
    "[\\[back to top\\]](#Data-Processing-and-Face-Patches)\n",
    "\n",
    "** The pre-process of face images: **\n",
    "\n",
    "1. Face Detection\n",
    "2. Face Alignment\n",
    "  - Facial landmark detection method: [Deep convolutional network cascade for facial point detection](#A.2.1-Facial-Landmark-Detection-Method)\n",
    "  - 5 facial points (two eye centers, nose tip, two mouth corners) used for alignment\n",
    "3. Face Cropping\n",
    "  - 39 x 31 x k (k = 3 for color image, k = 1 for gray image)\n",
    "  - 31 x 31 x k\n",
    "\n",
    "> We detect five facial landmarks, including the two eye centers, the nose tip, and the two mouth corners, with the facial point detection method proposed by Sun et al. [30]. (*[section] 3.2 Feature extraction*)\n",
    "\n",
    "** 60 patches of face images **\n",
    "\n",
    "- 3 scales, 5 global regions + 5 local regions, RGB or Gray\n",
    "  - 60 patches = 3 x (5+5) x 2\n",
    "\n",
    "** 100 patches of face images **\n",
    "\n",
    "- 5 scales, 5 global regions + 5 local regions, RGB or Gray\n",
    "  - 100 patches = 5 x (5+5) x 2\n",
    "\n",
    "![figure3](images/figure3.png)\n",
    "\n",
    "> Figure 3. Top: ten face regions of medium scales. The five regions in the top left are global regions taken from the weakly aligned faces, the other five in the top right are local regions centered around the five facial landmarks (two eye centers, nose tip, and two mouse corners). Bottom: three scales of two particular patches. (*[section] Figure 3. description*)\n",
    "\n",
    "> 60 face patches with ten regions, three scales, and RGB or gray channels. (*[section] 3.2 Feature extraction*)\n",
    "\n",
    "> 其中局部图像是关键点（每个图像一个关键点）居中，不同的区域大小和不同的尺度图像输入到CNN中，其CNN的结构可能会不相同，但是最后的特征的都是160维度，最后将所有的特征级联起来。 [\\[2\\]](#Reference)\n",
    "\n",
    "> \"The five regions in the top left are global regions taken from the weakly aligned faces\" 左上角那5个人脸块图像: 应该就是简单的根据人脸关键点做剪切 [\\[2\\]](#Reference)\n",
    "\n",
    "> \"three scales of two particular patches.\" 這裡的 scale 應該是指圖像\"顯示範圍\"\n",
    "\n",
    "```\n",
    "NOTE:\n",
    "\n",
    "5 global region 的個人想法：\n",
    "\n",
    "左圖: 39x31 \n",
    "\n",
    "右四圖: 31x31\n",
    "\n",
    "r-t-1: 取 邊界頂和眼高 之間為中心\n",
    "r-t-2: 取 邊界頂和鼻子 之間為中心\n",
    "r-d-1: 取 邊界頂和嘴巴 之間為中心\n",
    "r-d-2: 取 邊界頂和邊界底 之間為中心\n",
    "\n",
    "r-t, r-d 同一 size\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D.4.1 Face verification process ####\n",
    "\n",
    "[\\[back to top\\]](#Entire-Face-verification-process----ConvNet-+-Joint-Bayesian)\n",
    "\n",
    "image from http://blog.csdn.net/stdcoutzyx/article/details/41596663\n",
    "\n",
    "![face-verification](https://raw.githubusercontent.com/stdcoutzyx/Blogs/master/blogs/imgs/n2-process.png)\n",
    "\n",
    "- step1: feature extraction (ConvNet, or called DeepID model)\n",
    "- step2: feature recognition (Joint Bayesian)\n",
    "\n",
    "> we conduct feature extraction and recognition in two steps, with the first feature extraction step learned with the target of face identification, which is a much stronger supervision signal than verification. (*[section] 1.Introduction*)\n",
    "\n",
    "** Feature extraction: ConvNet **\n",
    "\n",
    "> We trained 60 ConvNets, each of which extracts two 160-dimensional DeepID vectors from a particular patch and its horizontally flipped counterpart. A special case is patches around the two eye centers and the two mouth corners, which are not flipped themselves, but the patches symmetric with them (for example, the flipped counterpart of the patch centered on the left eye is derived by flipping the patch centered on the right eye). (*[section] 3.2 Feature extraction*)\n",
    "\n",
    "> The total length of DeepID is 19, 200 (160 × 2 × 60), which is ready for the final face verification. (*[section] 3.2 Feature extraction*)\n",
    "\n",
    "** Face verification: Joint Bayesian **\n",
    "\n",
    "- method: [Bayesian face revisited: A joint formulation.](#A.1.1-current-face-verification-algorithms)\n",
    "  - with EM algorithm\n",
    "\n",
    "> We use the Joint Bayesian [8] technique for face verification based on the DeepID. (*[section] 3.3 Face verification*)\n",
    "\n",
    "> In face verification, our feature dimension is reduced to 150 by PCA before learning the Joint Bayesian model. (*[section] 4. Experiments*)\n",
    "\n",
    "> Sμ and Sε can be learned from data with EM algorithm. (*[section] 3.3 Face verification*)\n",
    "\n",
    "Keynote:\n",
    "\n",
    "1. 先用 8700 people 去訓練 DeepID model\n",
    "2. 用已訓練的 DeepID model 來取得 1477 people 的 DeepID\n",
    "3. 用 1477 people 的 DeepID 來訓練 Joint Bayesian\n",
    "4. 取得 LFW 的 6000 對 DeepID ，並測試已訓練的 Joint Bayesian\n",
    "  - input: 6000 pairs on LFW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D.4.2 ConvNet ####\n",
    "\n",
    "[\\[back to top\\]](#Entire-Face-verification-process----ConvNet-+-Joint-Bayesian)\n",
    "\n",
    "![figure2](images/figure2_ad.png)\n",
    "\n",
    "** ConvNet structure **\n",
    "\n",
    "```\n",
    "|C1| |C2|  |C3|  |C4| ->|F|  |        |\n",
    "  \\  /  \\  /  \\  /      | |->|soft-max|\n",
    "  |M1|  |M2|  |M3| ---->|C|  |        |\n",
    "\n",
    "Note:\n",
    "\n",
    "1. input size: 39(height)x31(width)xk or 31x31xk, k is 1 or 3\n",
    "2. FC 的輸出為 DeepID\n",
    "3. Cx 後面接 ReLU (activation function)\n",
    "4. C3: weights are locally shared in every 2×2 regions\n",
    "5. C4: weights are totally unshared\n",
    "6. back-propagation: stochastic gradient descent\n",
    "```\n",
    "\n",
    "> ConvNets contain four convolutional layers (with max-pooling) (*[section] 3.1 Deep ConvNets*)\n",
    "\n",
    "> followed by the fully-connected DeepID layer and the softmax output layer (*[section] 3.1 Deep ConvNets*)\n",
    "\n",
    "> The input is 39 × 31 × k for rectangle patches, and 31 × 31 × k for square patches, where k = 3 for color patches and k = 1 for gray patches. (*[section] 3.1 Deep ConvNets*)\n",
    "\n",
    "|Layer|kernel nums|kernel size|stride\n",
    "| ----|----|----- |----- |\n",
    "|C1|20|4X4|1|\n",
    "|M1|-|2X2|2|\n",
    "|C2|40|3X3|1|\n",
    "|M2|-|2X2|2|\n",
    "|C3|60|3X3|1|\n",
    "|M3|-|2X2|2|\n",
    "|C4|80|2X2|1|\n",
    "\n",
    "** Different feature extraction process: learn with face identification, not face verification **\n",
    "\n",
    "- two considerations below:\n",
    "  - effective features\n",
    "  - regularization, not over-fit\n",
    "\n",
    "> We propose an effective way to learn high-level over-complete features with deep ConvNets. (*[section] 1.Introduction*)\n",
    "\n",
    "> The ConvNets are learned to classify all the faces available for training by their identities, with the last hidden layer neuron activations as features (referred to as Deep hidden IDentity features or DeepID) (*[section] 1.Introduction*)\n",
    "\n",
    "> Classifying all the identities simultaneously instead of training binary classifiers as in [21, 2, 3] is based on two considerations. (*[section] 1.Introduction*)\n",
    "\n",
    "> This challenging task can make full use of the super learning capacity of neural networks to extract effective features for face recognition. (*[section] 1.Introduction*)\n",
    "\n",
    "> it implicitly adds a strong regularization to ConvNets, which helps to form shared hidden representations that can classify all the identities well. (*[section] 1.Introduction*)\n",
    "\n",
    "> Therefore, the learned high-level features have good generalization ability and do not over-fit to a small subset of training faces. (*[section] 1.Introduction*)\n",
    "\n",
    "** locally shared **\n",
    "\n",
    "> Weights in higher convolutional layers of our ConvNets are locally shared to learn different mid- or high-level features in different regions. (*[section] 3.1 Deep ConvNets*)\n",
    "\n",
    "> In the third convolutional layer, weights are locally shared in every 2 × 2 regions, while weights in the fourth convolutional layer are totally unshared. (*[section] 3.1 Deep ConvNets*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D.4.3 Joint Bayesian ###\n",
    "\n",
    "[\\[back to top\\]](#Entire-Face-verification-process----ConvNet-+-Joint-Bayesian)\n",
    "\n",
    "process:\n",
    "\n",
    "1. train the Joint Bayesian model with **\"DeepID\"**\n",
    "  - input: 150 dims **\"DeepID\"**\n",
    "2. test the Joint Bayesian model on LFW\n",
    "\n",
    "Note:\n",
    "\n",
    "- The face verification process of [Bayesian face revisited: A joint formulation.](#A.1.1-current-face-verification-algorithms) is: LBP + Joint Bayesian\n",
    "  - this paper use: DeepID + Joint Bayesian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D.4.4 \"neural network\"####\n",
    "\n",
    "[\\[back to top\\]](#Entire-Face-verification-process----ConvNet-+-Joint-Bayesian)\n",
    "\n",
    "- *\"neural network\"*: is a custom neural network for face verification\n",
    "\n",
    "![figure4_ad](images/figure4_ad.png)\n",
    "\n",
    "> We also train a neural network for verification and compare it to Joint Bayesian to see if other models can also learn from the extracted features and how much the features and a good face verification model contribute to the performance, respectively. (*[section] 3.3 Face verification*)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D.5.1 Experiments ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D.5.1.1 The classification ability of Multi-scale ConvNets ####\n",
    "\n",
    "[\\[back to top\\]](#Experiments)\n",
    "\n",
    "![figure5_ad](images/figure5_ad.png)\n",
    "\n",
    "Motivation:\n",
    "\n",
    "> We verify the effectiveness of directly connecting neurons in the third convolutional layer (after max-pooling) to the last hidden layer (the DeepID layer) (*[section] 4.1 Multi-scale ConvNets*)\n",
    "\n",
    "Process:\n",
    "\n",
    "```\n",
    "conventional method: only connect C4 to the last hidden layer\n",
    "\n",
    "multi-scale method: connect M3, C4 to the last hidden layer\n",
    "```\n",
    "\n",
    "- train 60 ConvNets with different 60 patches.\n",
    "  - with conventional method\n",
    "  - with multi-scale method\n",
    "- training set: 80% CelebFace (4349 people)\n",
    "- validation set: 10% images of each training person\n",
    "\n",
    "Result:\n",
    "\n",
    "> Figure 5 compares the top-1 validation set error rates of the 60 ConvNets learned to classify the 4349 classes of identities, either with or without the skipping layer. (*[section] 4.1 Multi-scale ConvNets*)\n",
    "\n",
    "> Allowing the DeepID to pool over multi-scale features reduces validation errors by an average of 4.72%. (*[section] 4.1 Multi-scale ConvNets*)\n",
    "\n",
    "> It actually also improves the final face verification accuracy from 95.35% to 96.05% when concatenating the DeepID from the 60 ConvNets and using Joint Bayesian for face verification. (*[section] 4.1 Multi-scale ConvNets*)\n",
    "\n",
    "Note:\n",
    "\n",
    "The face verification accuracy of concatenating from 60 ConvNets and using Joint Bayesian: (on LFW)\n",
    "\n",
    "- conventional method: 95.35%\n",
    "- multi-scale method: 96.05%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D.5.1.2 The effectiveness of the learned hidden representations for face verification ####\n",
    "\n",
    "[\\[back to top\\]](#Experiments)\n",
    "\n",
    "![figure6](images/figure6.png)\n",
    "\n",
    "![figure7](images/figure7.png)\n",
    "\n",
    "Motivation:\n",
    "\n",
    "> Classifying a large number of identities simultaneously is key to learning discriminative and compact hidden features. (*[section] 4.2 Learning effective features*)\n",
    "\n",
    "Process:\n",
    "\n",
    "1. train ConvNets with 136, 272, 544, 1087, 2175, 4349 people(classes), respectively.\n",
    "  - training set: 136, 272, 544, 1087, 2175, 4349 people on CelebFace.\n",
    "  - validation set: 10% images of each training person\n",
    "  - The input is a single patch covering the whole face in this experiment. (*[section] 4.2 Learning effective features*)\n",
    "  - There would be 6 learned ConvNets eventually.\n",
    "2. train Joint Bayesian or *\"neural network\"* model with features extracted from 6 learned ConvNets, respectively.\n",
    "  - training set: 20% CelebFace (1400 people)\n",
    "  - There would be 6 learned verification models eventually.\n",
    "3. test the verification models on LFW dataset.\n",
    "  - testing set: all LFW pairs (6000 pairs)\n",
    "\n",
    "Result:\n",
    "\n",
    "> More identity classes help to learn better hidden representations that can distinguish more people (discriminative) without increasing the feature length (compact). (*[section] 4.2 Learning effective features*)\n",
    "\n",
    "> we increase the identity classes for training exponentially (and output neuron numbers correspondingly) from 136 to 4349 while fixing the neuron numbers in all previous layers (the DeepID is kept to be 160 dimensional). (*[section] 4.2 Learning effective features*)\n",
    "\n",
    "> both Joint Bayesian and *\"neural network\"* improve linearly in verification accuracy when the identity classes double. (*[section] 4.2 Learning effective features*)\n",
    "\n",
    "> When identity classes increase 32 times from 136 to 4349, the accuracy increases by 10.13% and 8.42% for Joint Bayesian and neural networks, respectively, or 2.03% and 1.68% on average, respectively, whenever the identity classes double. (*[section] 4.2 Learning effective features*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D.5.1.3 The learned features extract identity information ####\n",
    "\n",
    "[\\[back to top\\]](#Experiments)\n",
    "\n",
    "![figure8](images/figure8.png)\n",
    "\n",
    "Motivation:\n",
    "\n",
    "> We find that faces of the same identity tend to have more commonly activated neurons (positive features being in the same position) than those of different identities. (*[section] 4.2 Learning effective features*)\n",
    "\n",
    "Process:\n",
    "\n",
    "1. train ConvNet\n",
    "  - training set: 80% CelebFace (4349 people)\n",
    "  - validation set: 10% images of each training person\n",
    "2. input three test pairs in LFW and retrieve corresponding 160-dims DeepID(features) extracted from each patch.\n",
    "3. rearrange \"activation pattern\"(sparsity pattern) as 5x32\n",
    "\n",
    "Result:\n",
    "\n",
    "> So the learned features extract identity information. (*[section] 4.2 Learning effective features*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D.5.1.4  Various face patches combination contributes to the performance ####\n",
    "\n",
    "[\\[back to top\\]](#Experiments)\n",
    "\n",
    "![figure9](images/figure9.png)\n",
    "\n",
    "Motivation:\n",
    "\n",
    "> We evaluate how much combining features extracted from various face patches would contribute to the performance. (*[section] 4.3 Over-complete representation*)\n",
    "\n",
    "> We train the face verification model with features from k patches (k = 1, 5, 15, 30, 60). (*[section] 4.3 Over-complete representation*)\n",
    "\n",
    "Process:\n",
    "\n",
    "1. train the ConvNet, face verification model with features from single patch (k=1)\n",
    "  - report the best-performing single patch\n",
    "2. train the ConvNet, face verification model with features from **global color patches in a single scale** (k=5)\n",
    "3. train the ConvNet, face verification model with features from **all the global color patches** (k=15)\n",
    "4. train the ConvNet, face verification model with features from **all the color patches** (k=30)\n",
    "5. train the ConvNet, face verification model with features from **all the patches** (k=60)\n",
    "\n",
    "- training set: 80% CelebFace (4349 people)\n",
    "- validation set: 10% images of each training person\n",
    "\n",
    "Result:\n",
    "\n",
    "> We report the best-performing single patch (k = 1), the global color patches in a single scale (k = 5), all the global color patches (k = 15), all the color patches (k = 30), and all the patches (k = 60). (*[section] 4.3 Over-complete representation*)\n",
    "\n",
    "> As shown in Figure 9, adding more features from various regions, scales, and color channels consistently improves the performance. (*[section] 4.3 Over-complete representation*)\n",
    "\n",
    "> Combing 60 patches increases the accuracy by 4.53% and 5.27% over best single patch for Joint Bayesian and neural networks, respectively. (*[section] 4.3 Over-complete representation*)\n",
    "\n",
    "> We achieve 96.05% and 94.32% accuracy using Joint Bayesian and neural networks, respectively. (*[section] 4.3 Over-complete representation*)\n",
    "\n",
    "Note:\n",
    "\n",
    "Process of creating DeepID concatenation (60 ConvNets):\n",
    "\n",
    "1. Concatenate 60x2 DeepIDs(origin, horizontally flipped counterpart) from 60 ConvNets => called DeepID'\n",
    "  - dims of DeepID' = 60x(2x160) = 19200 dims\n",
    "2. Do PCA (reduce dims of DeepID' to 150 dims) => this is final \"DeepID concatenation\"!\n",
    "  - dims of DeepID concatenation = 150 dims\n",
    "\n",
    "The face verification accuracy of concatenating from 60 ConvNets: (on LFW)\n",
    "\n",
    "- using Joint Bayesian: 96.05%\n",
    "- using *\"neural networks\"*: 94.32%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D.5.1.5 Method comparison ####\n",
    "\n",
    "[\\[back to top\\]](#Experiments)\n",
    "\n",
    "![table1](images/table1.png)\n",
    "\n",
    "![figure10](images/figure10.png)\n",
    "\n",
    "Process:\n",
    "\n",
    "1. train and test DeepID on CelebFaces\n",
    "  - 60 ConvNets, Joint Bayesian\n",
    "2. train and test DeepID on CelebFace+\n",
    "  - 100 ConvNets, Joint Bayesian\n",
    "3. train and test DeepID on CelebFace+ & TL (Transfer learning algorithm)\n",
    "  - 100 ConvNets, Joint Bayesian, TL\n",
    "\n",
    "Result:\n",
    "\n",
    "1. test accuracy of DeepID on CelebFace (60 ConvNets)\n",
    "  - 96.05%\n",
    "2. test verification of DeepID on CelebFace+ (100 ConvNets)\n",
    "  - 97.20%\n",
    "3. test verification of DeepID on CelebFace+ (100 ConvNets) & TL\n",
    "  - 97.45%\n",
    "\n",
    "> we enlarge the CelebFaces dataset to CelebFaces+, which contains 202, 599 face images of 10, 177 celebrities. (*[section] 4.4 Method comparison*)\n",
    "\n",
    "> People in CelebFaces+ and LFW are mutually exclusive. (*[section] 4.4 Method comparison*)\n",
    "\n",
    "> We randomly choose 8700 people from CelebFaces+ to learn the DeepID, and use the remaining 1477 people to learn Joint Bayesian for face verification. (*[section] 4.4 Method comparison*)\n",
    "\n",
    "> we increase the patch number to 100 by using five different scales of patches instead of three. (*[section] 4.4 Method comparison*)\n",
    "\n",
    "> This results in a 32,000-dimensional DeepID feature vector, which is then reduced to 150 dimensions by PCA. Joint Bayesian learned on this 150-dimensional feature vector achieves 97.20% test accuracy on LFW. (*[section] 4.4 Method comparison*)\n",
    "\n",
    "> Due to the difference in data distributions, models well fitted to CelebFaces+ may not have equal generalization ability on LFW. (*[section] 4.4 Method comparison*)\n",
    "\n",
    "> Cao et al. [6] proposed a practical transfer learning algorithm to adapt the Joint Bayesian model from the source domain to the target domain. (*[section] 4.4 Method comparison*)\n",
    "\n",
    "> We implemented their algorithm by using the 1477 people from CelebFaces+ as the source domain data and nine out of ten folders from LFW as the target domain data for transfer learning Joint Bayesian, and conduct ten- fold cross validation on LFW. (*[section] 4.4 Method comparison*)\n",
    "\n",
    "Note:\n",
    "\n",
    "- Transfer learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\\[back to top\\]](#Comparison-of-current-face-verification-algorithms)\n",
    "\n",
    "#### A.1.1 current face verification algorithms ####\n",
    "\n",
    "The current best-performing face verification algorithms:\n",
    "\n",
    "- [9] D. Chen, X. Cao, F. Wen, and J. Sun. [Blessing of dimensionality: High-dimensional feature and its efficient compression for face verification.](http://www.msr-waypoint.net/en-us/um/people/jiansun/papers/CVPR13_HighDim.pdf) In Proc. CVPR, 2013.\n",
    "- [29] K. Simonyan, O. M. Parkhi, A. Vedaldi, and A. Zisserman. [Fisher vector faces in the wild.](https://www.robots.ox.ac.uk/~vgg/publications/2013/Simonyan13/simonyan13.pdf) In Proc. BMVC, 2013.\n",
    "- [6] X. Cao, D. Wipf, F. Wen, G. Duan, and J. Sun. [A practical transfer learning algorithm for face verification.](http://research.microsoft.com/pubs/202192/TransferLearning.pdf) In Proc. ICCV, 2013\n",
    "- [8] D. Chen, X. Cao, L. Wang, F. Wen, and J. Sun. [Bayesian face revisited: A joint formulation.](http://research.microsoft.com/pubs/192105/JointBayesian.pdf) In Proc. ECCV, 2012. [1](#D.4.1-Face-verification-process) [2](#D.4.3-Joint-Bayesian)\n",
    "\n",
    "#### A.2.1 Facial Landmark Detection Method ####\n",
    "\n",
    "- [30] Y.Sun, X.Wang, andX.Tang. [Deep convolutional network cascade for facial point detection.](http://www.ee.cuhk.edu.hk/~xgwang/papers/sunWTcvpr13.pdf) In Proc. CVPR, 2013 [1](#D.3.1-Data-Processing-and-Face-Patches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state-of-the-art: 最先進的\n",
    "- unconstrained condition: 未受人為控制的**實際環境**\n",
    "- over-complete: 過度完全性（**過多**），相對於 compact\n",
    "  - [信號處理案例：對數據壓縮等應用是不利的](https://books.google.com.tw/books?id=TfoCbM_9vzYC&pg=PA106&lpg=PA106&dq=overcomplete+%E6%84%8F%E7%BE%A9&source=bl&ots=1iepO7zIaX&sig=yY3pv9y4xijk_U0H689AW_pTfxM&hl=zh-TW&sa=X&ved=0ahUKEwims-ado7_LAhUELKYKHVcgAuEQ6AEISDAH#v=onepage&q=overcomplete%20%E6%84%8F%E7%BE%A9&f=false)\n",
    "- feature extraction cascade: 指從 low-level 到 high-level 的特徵萃取過程\n",
    "- L2 distance: L2 norm, a euclidean distance between points\n",
    "  - [wiki: Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)\n",
    "  - [mathisfunforum](http://www.mathisfunforum.com/viewtopic.php?id=17995)\n",
    "- LBP: Local binary patterns\n",
    "  - [wiki: 局部二值模式](https://zh.wikipedia.org/wiki/%E5%B1%80%E9%83%A8%E4%BA%8C%E5%80%BC%E6%A8%A1%E5%BC%8F)\n",
    "- softMax: [UFLDL: SoftMax](http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92)\n",
    "- Joint Bayesian, EM:\n",
    "  - [【人脸识别】人脸验证算法Joint Bayesian详解及实现（Python版）](http://blog.csdn.net/cyh_24/article/details/49059475)\n",
    "  - [ Bayesian face revisited : a joint formulation 笔记 ](http://blog.csdn.net/csyhhb/article/details/46300001)\n",
    "  - [已知两个高斯分布及他们的关系，如何求条件期望? ](http://www.zhihu.com/question/28086678)\n",
    "  - [Bayesian Face Revisited: A Joint Formulation 算法流程图 ](http://blog.csdn.net/hqbupt/article/details/37758627)\n",
    "- Dropout:\n",
    "- gradient diffusion\n",
    "- Transfer learning [paper: a pratical transfer learning algorithm for face verification]:\n",
    "- K-fold cross-validation: K次交叉驗證，初始採樣分割成K個子樣本，一個單獨的子樣本被保留作為驗證模型的數據，其他K-1個樣本用來訓練。交叉驗證重複K次，每個子樣本驗證一次，平均K次的結果或者使用其它結合方式，最終得到一個單一估測。這個方法的優勢在於，同時重複運用隨機產生的子樣本進行訓練和驗證，每次的結果驗證一次，10次交叉驗證是最常用的。\n",
    "  - https://zh.wikipedia.org/wiki/%E4%BA%A4%E5%8F%89%E9%A9%97%E8%AD%89#K-fold_cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource ##\n",
    "\n",
    "- [數據堂](http://datatang.com/)\n",
    "  - 各式數據下載\n",
    "- [AlfredXiangWu/face_verification_experiment](https://github.com/AlfredXiangWu/face_verification_experiment)\n",
    "- [RiweiChen/DeepFace](https://github.com/RiweiChen/DeepFace)\n",
    "- [cyh24/Joint-Bayesian](https://github.com/cyh24/Joint-Bayesian)\n",
    "- [RiweiChen/FaceTools](https://github.com/RiweiChen/FaceTools)\n",
    "- [【云计算虚拟化】Docker的基本命令使用 ](http://blog.csdn.net/chenriwei2/article/details/50250923)\n",
    "- [【云计算虚拟化】基于docker的caffe环境搭建 ](http://blog.csdn.net/chenriwei2/article/details/50250685)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference ##\n",
    "\n",
    "[1] 張雨石, [DeepID人脸识别算法之三代](http://blog.csdn.net/stdcoutzyx/article/details/42091205), 2014-12-23\n",
    "\n",
    "[2] Riwei Chen, [【Caffe实践】基于Caffe的人脸识别实现 - DeepID](http://blog.csdn.net/chenriwei2/article/details/49500687), 2015-11-01\n",
    "\n",
    "[3] Riwei Chen, [【深度学习论文笔记】Deep Learning Face Representation from Predicting 10,000 Classes ](http://blog.csdn.net/chenriwei2/article/details/31415069), 2015-03-24 [1](#D.3.1-Data-Processing-and-Face-Patches)\n",
    "\n",
    "[4] CSDNcloud, [專訪DeepID發明者孫禕：關於深度學習與人臉算法的深層思考](http://wechat.kanfb.com/archives/129700), 2015-11-27\n",
    "\n",
    "[5] 仙道菜, [【人脸识别】人脸验证算法Joint Bayesian详解及实现（Python版）](http://blog.csdn.net/cyh_24/article/details/49059475), 2015-10-12\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
